{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Clustering Notebook</h1>\n",
    "\n",
    "This notebook includes the whole process of agglomerative heirarhcical clustering including all visualizations in the presentation. In this notebook is scraping and downloading the data, cleaning it, explaining how clustering works, and picking hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "! pip3 install yfinance -q\n",
    "! pip3 install numpy -q\n",
    "! pip3 install seaborn -q\n",
    "! pip install pandas-datareader -q\n",
    "! pip install networkx -q\n",
    "! pip install --upgrade nbformat -q\n",
    "! pip install sklearn -q\n",
    "! pip install scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "''\n",
    "# this workaround let's us override pandas_datareader with yfinance,\n",
    "# although we can also use yfinance directly\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as web; yf.pdr_override()\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import cm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Web Scraping (given)</h3>\n",
    "scraping from wikipedia and yahoo finance to get companies from S&P 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our time series is from the past year 2022-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "start_date = '2022-01-01'\n",
    "end_date  = '2022-12-31'\n",
    "\n",
    "data_dir = '/Users/sofia/Onedrive/Documents/Github/BTT-JPMC-1'\n",
    "dates = '_{}--{}'.format(start_date, end_date)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we're getting the symbols from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data    = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies#S%26P_500_component_stocks')\n",
    "table_symbol   = data[0]\n",
    "symbols = list(table_symbol.Symbol.values) #ticker symbols\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each ticker symbol in our list we're going to get the closing price data from 01-01-2022 to 12-31-2022\n",
    "make that into a dataframe df and store those in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "symbols = sorted(symbols)\n",
    "print(\"Downloading {} files\".format(len(symbols)))\n",
    "for i, symbol in enumerate(symbols):\n",
    "    try:\n",
    "        df = web.get_data_yahoo(symbol, start_date, end_date,)\n",
    "        # df = web.DataReader(symbol,'yahoo', start_date, end_date)\n",
    "        df = df[['Adj Close']]\n",
    "        df.to_csv(os.path.join(data_dir, \"{}.csv\".format(symbol)))\n",
    "    except KeyError:\n",
    "      print(\"Error for {}\".format(symbol))\n",
    "      pass\n",
    "print(\"Stored {} files\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pre-processing the data</h2>\n",
    "Now we have 500 csv files and we want to make one big one, indexed with all dates acorss 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "index = pd.date_range(start=start_date, end=end_date, freq='D')     # initialize an empty DateTime Index and so each index is a date over the year\n",
    "df_price = pd.DataFrame(index=index, columns=symbols)               # initialize empty dataframes\n",
    "\n",
    "for symbol in symbols:\n",
    "    symbol_df = pd.read_csv(os.path.join(data_dir, symbol+\".csv\")).set_index('Date')\n",
    "    symbol_df.index = pd.to_datetime(symbol_df.index)\n",
    "    df_price[symbol] = symbol_df['Adj Close']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we;re going to remove any rows (days) where the stock price is nan, these would be like holidays or weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Let's drop the dates where all the stocks are NaNs, ie., weekends/holidays where no trading occured\n",
    "df_price.dropna(how='all', inplace=True)\n",
    "df_price.dropna(inplace=True, axis=1)\n",
    "True in pd.isna(df_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we're going to find the price percentage change from day to day with the closing price. This is called the daily returns. we're using daily returns because if we use the raw price data our results will just clump all the big companies (with high stock prices) together, what we're looking for is to see which companies move similarly in the market, not ones that are similalrly priced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df_price_pct = df_price.pct_change()[1:]\n",
    "df_price_pct.to_csv(os.path.join(data_dir, \"prices_pct.csv\"), index_label='date')\n",
    "df_price_pct = df_price.pct_change().dropna(how='all')\n",
    "df_price_pct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Outlier detour</h2>\n",
    "This is not explored in our presentation but if we had more time we would look into the affect outliers have on our data. There seems to be some days and companies where the returns are abnormally high or low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below shows which companies are outliers across the whole year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df_price_plotting = df_price_pct\n",
    "df_2 = df_price_plotting.mean(axis=0)\n",
    "avg_per_company = list(df_2.values)\n",
    "plt.boxplot(avg_per_company)\n",
    "plt.title('average price pct change per company 2022-2023')\n",
    "plt.ylabel('price pct change averaged over the past year')\n",
    "plt.show()\n",
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below shows which days in the time series are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df_price_plotting['average'] = df_price_plotting.mean(axis=1)\n",
    "#average across all companies per day\n",
    "# Creating plot\n",
    "plt.boxplot(df_price_plotting['average'])\n",
    "# show plot\n",
    "plt.title('average price pct change per day')\n",
    "plt.ylabel('price pct change averaged over the past year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now this is an overall graph showing how every company on the index did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "plt.title('S&P price change histories during march 2022 (percent)')\n",
    "plt.plot(df_price_pct, linewidth=0.10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df_price_plotting = df_price['2022-04-01':'2022-04-30']\n",
    "plt.title('S&P price change april 2022')\n",
    "plt.plot(df_price_plotting['MSFT'], linewidth=1, color=\"red\", label=\"Microsoft\")\n",
    "plt.plot(df_price_plotting['ADSK'], linewidth=1, color=\"blue\", label=\"Autodesk\")\n",
    "plt.plot(df_price_plotting['VRSK'], linewidth=1, color=\"green\", label=\"Verisk\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df_price_plotting = df_price_pct['2022-04-01':'2022-04-30']\n",
    "plt.title('S&P price change april 2022')\n",
    "plt.plot(df_price_plotting['MSFT'], linewidth=1, color=\"red\", label=\"Microsoft\")\n",
    "plt.plot(df_price_plotting['ADSK'], linewidth=1, color=\"blue\", label=\"Hilton WW\")\n",
    "plt.plot(df_price_plotting['VRSK'], linewidth=1, color=\"green\", label=\"Verisk\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print('daily returns correlation between autodesk and microsoft', df_price_pct.corr().loc['MSFT', 'ADSK'])\n",
    "print('daily returns correlation between autodesk and verisk ',df_price_pct.corr().loc['ADSK', 'VRSK'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we're going to take those returns and make the dataframe into a correlation matrix. showing how the daily returns of each company is related to the other 500, we want companies that move similarly in the market to be highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "price_corr = df_price_pct.corr()\n",
    "price_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "sns.heatmap(abs(price_corr)).set_title(\"Correlation heatmap for Price Changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing how correlation is related to sector\n",
    "\n",
    "the green companies are utulities sector while orange are real estate companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "columns = ['CMS', 'DTE', 'LNT', 'SO', 'WEC', 'CPT', 'UDR', 'ESS', 'FRT', 'MAA']\n",
    "plotting_df = df_price_pct[columns]\n",
    "plotting_df = plotting_df.corr()\n",
    "heatmap = sns.heatmap(abs(plotting_df), cmap=\"Blues\")\n",
    "\n",
    "# Define the companies to be colored differently\n",
    "green_companies = ['CMS', 'DTE', 'LNT', 'SO', 'WEC']\n",
    "red_companies = ['CPT', 'UDR', 'ESS', 'FRT', 'MAA']\n",
    "\n",
    "# Customize the text color on the x-axis\n",
    "for i, tick_label in enumerate(heatmap.get_xticklabels()):\n",
    "    company = columns[i]\n",
    "    if company in green_companies:\n",
    "        tick_label.set_color(\"green\")\n",
    "    elif company in red_companies:\n",
    "        tick_label.set_color(\"orange\")\n",
    "\n",
    "# Customize the text color on the y-axis\n",
    "for i, tick_label in enumerate(heatmap.get_yticklabels()):\n",
    "    company = columns[i]\n",
    "    if company in green_companies:\n",
    "        tick_label.set_color(\"green\")\n",
    "    elif company in red_companies:\n",
    "        tick_label.set_color(\"orange\")\n",
    "\n",
    "# Set the title and show the plot\n",
    "heatmap.set_title(\"Daily Returns Heatmap For Utilities Industry vs Real Estate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>\n",
    "heirarhcical agglomerative clustering is a bottom up unsupervised learning approach that creates clusters and visualizes them in a tree-like structure (called dendrogram) this method is useful if you don't know the number of clusters you want beforehand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clustering Data</h3>\n",
    "1-abs(df_price_pct.corr().values)\n",
    "\n",
    "is a lot to unpack lol. df_price_pct.corr() is the correlation matrix of daily returns for all 500 companies related to each other. We then find the absolute value of those numbers because a strong negative correlation is just as important as a strong positive correlation. We're doing 1- all of that because heirarichical clustering is typically made for distance matrices not similarity matrices. heirarhcial clustering will group together two nodes if they're distance is lower, because that means the two nodes are closer together, to acheive this with similarity we have to take the inverse to highly correlated companies will have a lower number, meaning they are \"closer\" together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our clustering is based off of the below dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "dissimilarity = 1- abs(df_price_pct.corr().values)\n",
    "\n",
    "#diff types of linkage determine how the clusters are merged \n",
    "# single - merge clusters closest together , \n",
    "# complete - merge clusters furthest\n",
    "#ward linkage attempts to minimize variance within clusters after being merged\n",
    "#single linkage maximized the cophenetic correlation the others were around .34 and .38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using our dissimilarity matrix we create a linkage matrix (Z)\n",
    "The linkage matrix is what clusters our data points together. Each row in the matrix describes a merge of two clusters, 4 values in each row. the first two values are the indeces of the two datapoints merged, the third value is the disimilarity value they were merged at and the 4th value is how many clusters were merged (always 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linkage method is how you calculate which companies are \"closer\" together in the dissimilariry matrix. Each time a merge is performed the dissimilarity matrix is updated with new values from the merged cluster to all the remaining data points.\n",
    "<b>Single linkage</b> - recalculates the dissimiarlity (or distance) matrix using the minimum distance between clusters or points\n",
    "<b>complete linkage</b> - recalculates the distance matrix using the maximum distance between clusters or points\n",
    "<b>Average linkage</b> - recalculates the distance matrix using the average distance between clusters\n",
    "<b>Ward linkage</b> - recalculates distance based on how merging those points will increase the sum of squares. The point is to create clusters that minimize variance and are very compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "Z = linkage(squareform(dissimilarity), method=\"ward\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the linkage matrix is made (Z) you can visualize the results with a dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "dendrogram = sch.dendrogram(Z)\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('dissimilarity')\n",
    "plt.title('Dendrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this dendrogram shows you where all 500 points are merged together at (xaxis is all companies) but to actually get the clusters you need to choose a threshold to cut the dendrogram at. If you cut the above dendrogram at 2.9 dissimialrity there would be 4 clusters, if you cut at 4.5 there will be 2 clusters etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "labels = fcluster(Z, 3.5, criterion='distance')\n",
    "num_clusters = len(set(labels))\n",
    "\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "cluster_sizes = [np.sum(labels == label) for label in unique_labels]\n",
    "\n",
    "print('there are ', num_clusters, ' clustres and these are the sizes of each', cluster_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df_labels = pd.DataFrame({'Company': df_price_pct.columns, 'Cluster_Label': labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>There are two hyperparameters: number of clusters and linkage</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find the linkage we looked for linkage methods that would provide a balanced number of clusters so not [499, 1, 1] but maybe [300, 100, 100].\n",
    "Based off of theory and domain knowledge we can assume the ward linkage would be best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so at various thresholds (number of clusters), I used different linkage methods and found the distributions of nodes the clusters. In theory if the standard deviation of these spreads is smaller then that means the sizes of the clusters are less extreme and more balanced.\n",
    "\n",
    "finding balanced clusters is very important because if you have [498,1,1] or [1,1,1,2,1,1,1,1,...1,1,1,1] then its the same as not clustering at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these thresholds are the thresholds for each linkage method that at which you can get 2-15, 20, & 25 clusters from. All of the linkage methods calculate distance differently si theyre all on different scales. this is why i needed threshold numbers for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "thresholds_ward = [4.0,3.5, 3.0, 2.24,2.2, 2.1,1.981,1.980, 1.7,1.35,1.2, 1.04] #1.35 is threshold for 15 1.2 is 20 clusters\n",
    "thresholds_comp = [.999,.995,.990, .965,.96,.94,.91,.87,.84,.80,.76, .721 ] #.80 is threshold for 15, 0.76 is 20\n",
    "thresholds_single = [.55,.54, .53, .51,.50, .49,.486,.485,.480,.471,.4475,.43 ] #.471 is threshold for 15, 0.4475 for 20 clusters\n",
    "thresholds_average = [.80,.79,.73,.71,.70, .69,.675,.66,.659 ,.6425,.614,.6] #.6425 is theshold for 15, 0.614 is for 20 clsuters\n",
    "ward_std = []\n",
    "comp_std = []\n",
    "aver_std = []\n",
    "sing_std = []\n",
    "num_cluster_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this large for loop finds out the distribution of clusters for each linkage method at varying thresholds (see above cell)\n",
    "displays result in bar plot, we want to minimize the standard deviation. low standard deviation means the distributions of the cluster sizes are close together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i, j,k,l in zip(thresholds_ward, thresholds_comp, thresholds_single, thresholds_average):\n",
    "    Z_ward = linkage(squareform(dissimilarity), method=\"ward\", optimal_ordering=True)\n",
    "    Z_comp = linkage(squareform(dissimilarity), method=\"complete\", optimal_ordering=True)\n",
    "    Z_sing = linkage(squareform(dissimilarity), method=\"single\", optimal_ordering=True)\n",
    "    Z_aver = linkage(squareform(dissimilarity), method=\"average\", optimal_ordering=True)\n",
    "    \n",
    "\n",
    "\n",
    "    labels_ward = fcluster(Z_ward, i, criterion=\"distance\")\n",
    "    labels_comp = fcluster(Z_comp, j, criterion=\"distance\")\n",
    "    labels_sing = fcluster(Z_sing, k, criterion=\"distance\")\n",
    "    labels_aver = fcluster(Z_aver, l, criterion=\"distance\")\n",
    "\n",
    "    unique_labels_ward = np.unique(labels_ward)\n",
    "    cluster_sizes_ward = [np.sum(labels_ward == label) for label in unique_labels_ward]\n",
    "\n",
    "    unique_labels_comp = np.unique(labels_comp)\n",
    "    cluster_sizes_comp = [np.sum(labels_comp == label) for label in unique_labels_comp]\n",
    "\n",
    "    unique_labels_sing = np.unique(labels_sing)\n",
    "    cluster_sizes_sing = [np.sum(labels_sing == label) for label in unique_labels_sing]\n",
    "\n",
    "    unique_labels_aver = np.unique(labels_aver)\n",
    "    cluster_sizes_aver = [np.sum(labels_aver == label) for label in unique_labels_aver]\n",
    "\n",
    "\n",
    "    # Calculate the standard deviation for each method you want the lowest standard devation to make the clusters balanced\n",
    "    ward_std_dev = np.std(cluster_sizes_ward)\n",
    "    comp_std_dev = np.std(cluster_sizes_comp)\n",
    "    ward_std.append(ward_std_dev)\n",
    "    comp_std.append(comp_std_dev)\n",
    "    num_cluster_plot.append(max(labels_ward))\n",
    "\n",
    "    sing_std_dev = np.std(cluster_sizes_sing)\n",
    "    aver_std_dev = np.std(cluster_sizes_aver)\n",
    "    sing_std.append(sing_std_dev)\n",
    "    aver_std.append(aver_std_dev)\n",
    "    \n",
    "\n",
    "    print('For ward linkage there are', max(labels_ward), 'clusters and these are the sizes of each', cluster_sizes_ward, ' the standard deviation is ', ward_std_dev)\n",
    "    print('For complete linkage there are', max(labels_comp), 'clusters and these are the sizes of each', cluster_sizes_comp, ' the standard deivation is ', comp_std_dev)\n",
    "    print('For single linkage there are', max(labels_sing), 'clusters and these are the sizes of each', cluster_sizes_sing, ' the standard deivation is ', sing_std_dev)\n",
    "    print('\\n')\n",
    "    \n",
    "bar_width = 0.15\n",
    "opacity = 0.8\n",
    "index = np.arange(len(thresholds_ward))\n",
    "\n",
    "plt.bar(index, ward_std, bar_width, label='Ward clusters Std', alpha=opacity)\n",
    "plt.bar(index + bar_width, comp_std, bar_width, label='Complete clusters Std', alpha=opacity)\n",
    "plt.bar(index+bar_width+bar_width,sing_std, bar_width, label = \"Single clusters std\", alpha = opacity)\n",
    "plt.bar(index+bar_width+bar_width+bar_width, aver_std, bar_width, label=\"Average clusters std\", alpha = opacity)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title('Standard Deviations of cluster distributions for diff linkage methods')\n",
    "plt.xticks(index + bar_width / 2, [str(clust) for clust in num_cluster_plot])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above bar plot shows that no matter how many clusters are chosen, ward always gives the most balanced distributino of clsuter sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we'll use ward linkage to make our linkage matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We have linkage, lets choose a threshold/h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're going to use the metric silhouette score to find the best threshold to cut our dendrogram at. I'm going to plot it and find where silhouette score is the highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "silhouette score is a metric that values compact and separated clusters so small clusters that are far apart. we want a high silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0.1, 4.5, 0.1)\n",
    "\n",
    "\n",
    "#im going to store the results of this for loop into a dataframe so i can compare everything better\n",
    "columns = ['threshold','Silhouette', 'num clusters']\n",
    "price_corr_eval = pd.DataFrame(columns=columns)\n",
    "        \n",
    "for t in thresholds:\n",
    "        Z = linkage(squareform(dissimilarity), method = \"ward\")\n",
    "        labels = fcluster(Z, t, criterion='distance')\n",
    "        silhouette_avg = silhouette_score(dissimilarity, labels, metric='precomputed')\n",
    "        #silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        c, _ = cophenet(Z, squareform(dissimilarity))\n",
    "        #cophenet_correlations.append(c)\n",
    "        ch = calinski_harabasz_score(dissimilarity, labels)\n",
    "        num_clusters = max(labels)\n",
    "        new_row = [t, silhouette_avg, num_clusters]\n",
    "        price_corr_eval.loc[len(price_corr_eval)] = new_row\n",
    "        #print(f\"Silhouette Score for threshold {t}: {silhouette_avg} and cophenet corr for link method {l}: {c}\")\n",
    "price_corr_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "plt.plot(price_corr_eval['threshold'], price_corr_eval['Silhouette'])\n",
    "plt.xticks(np.arange(0, 5.5, 0.5))\n",
    "# Add labels and title\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.title('num clust vs silhouette score for ward linkage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myvenv2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/sofia/Downloads/BTT-JPMC-1-1/myvenv2/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "plt.plot(price_corr_eval['num clusters'], price_corr_eval['Silhouette'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.title('num clust vs silhouette score for ward linkage from 0-500')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
